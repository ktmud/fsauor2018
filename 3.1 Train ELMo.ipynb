{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use [ELMo](https://allennlp.org/elmo), we can either use the [pre-trained models](https://github.com/HIT-SCIR/ELMoForManyLangs), or train a set of word representations of our own.\n",
    "\n",
    "This notebook shows how to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-11-15 19:54:52,426 DEBUG: Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "2018-11-15 19:54:53,624 DEBUG: Dumping model to file cache /var/folders/6r/772b4sy16rg94jhq1fskv9fc0000gn/T/jieba.cache\n",
      "Loading model cost 1.294 seconds.\n",
      "2018-11-15 19:54:53,723 DEBUG: Loading model cost 1.294 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-11-15 19:54:53,725 DEBUG: Prefix dict has been built succesfully.\n",
      "2018-11-15 19:54:53,728 INFO: Read cache data/train/sentiment_analysis_trainingset.csv.segged_sample_None.tsv..\n",
      "2018-11-15 19:54:56,978 INFO: Read cache data/validate/sentiment_analysis_validationset.csv.segged_sample_None.tsv..\n",
      "2018-11-15 19:54:57,496 INFO: Read cache data/test-a/sentiment_analysis_testa.csv.segged_sample_None.tsv..\n",
      "2018-11-15 19:54:57,874 INFO: Read cache data/test-b/sentiment_analysis_testb.csv.segged_sample_None.tsv..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from config import validate_data_path, train_data_path, testa_data_path, testb_data_path\n",
    "from fgclassifier.utils import read_csv\n",
    "from fgclassifier.embedding import count_to_corpus\n",
    "\n",
    "# read_csv reads by default the tokenized text\n",
    "df_train = read_csv(train_data_path)\n",
    "df_valid = read_csv(validate_data_path)\n",
    "df_testa = read_csv(testa_data_path)\n",
    "df_testb = read_csv(testb_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼å¼ å¼ ï¼Œ èŒæ­» äºº çš„ æ£’æ£’ç³– ï¼Œ ä¸­ äº† å¤§ä¼— ç‚¹è¯„ çš„ éœ¸ç‹é¤ ï¼Œ å¤ª å¯çˆ± äº†\n",
      "ä¸€ç›´ å°± å¥½å¥‡ è¿™ä¸ª æ£’æ£’ç³– æ˜¯ æ€ä¹ˆ ä¸ª ä¸œè¥¿ ï¼Œ å¤§ä¼— ç‚¹è¯„ ç»™ äº† æˆ‘ è¿™ä¸ª åœŸè€å†’ ä¸€ä¸ª è§è¯† çš„ æœºä¼š\n",
      "çœ‹ ä»‹ç» æ£’æ£’ç³– æ˜¯ ç”¨ å¾·å›½ ç³– åš çš„ ï¼Œ ä¸ä¼š å¾ˆç”œ ï¼Œ ä¸­é—´ çš„ ç…§ç‰‡ æ˜¯ ç³¯ç±³ çš„ ï¼Œ èƒ½ é£Ÿç”¨ ï¼Œ çœŸæ˜¯å¤ª é«˜ç«¯ å¤§æ°” ä¸Šæ¡£æ¬¡ äº† ï¼Œ è¿˜ å¯ä»¥ ä¹° è´è¶ç»“ æ‰å£ ï¼Œ é€äºº å¯ä»¥ ä¹° ç¤¼ç›’\n",
      "æˆ‘ æ˜¯ å…ˆ æ‰“ çš„ å–å®¶ ç”µè¯ ï¼Œ åŠ  äº† å¾®ä¿¡ ï¼Œ ç»™ å–å®¶ ä¼  çš„ ç…§ç‰‡\n",
      "ç­‰ äº† å‡ å¤© ï¼Œ å–å®¶ å°± å‘Šè¯‰ æˆ‘ å¯ä»¥ å–è´§ äº† ï¼Œ å» å¤§å®˜ å±¯ é‚£å– çš„\n",
      "è™½ç„¶ è¿ å–å®¶ çš„ é¢ éƒ½ æ²¡ è§åˆ° ï¼Œ ä½†æ˜¯ è¿˜æ˜¯ è°¢è°¢ å–å®¶ é€ æˆ‘ è¿™ä¹ˆ å¯çˆ± çš„ ä¸œè¥¿ ï¼Œ å¤ª å–œæ¬¢ äº† ï¼Œ è¿™ å“ª èˆå¾—åƒ å•Š\n",
      "ç¬¬ä¸‰æ¬¡ å‚åŠ  å¤§ä¼— ç‚¹è¯„ ç½‘ éœ¸ç‹é¤ çš„ æ´»åŠ¨\n",
      "è¿™å®¶ åº— ç»™ äºº æ•´ä½“ æ„Ÿè§‰ ä¸€èˆ¬\n",
      "é¦–å…ˆ ç¯å¢ƒ åªèƒ½ ç®— ä¸­ç­‰ ï¼Œ å…¶æ¬¡ éœ¸ç‹é¤ æä¾› çš„ èœå“ ä¹Ÿ ä¸æ˜¯ å¾ˆå¤š ï¼Œ å½“ç„¶ å•†å®¶ ä¸ºäº† é¿å… å‚åŠ  éœ¸ç‹é¤ åƒä¸é¥± çš„ ç°è±¡ ï¼Œ ç»™ æ¯æ¡Œ éƒ½ æä¾› äº† è‡³å°‘ å…­ä»½ ä¸»é£Ÿ ï¼Œ æˆ‘ä»¬ é‚£æ¡Œ éƒ½ æä¾› äº† ä¸¤ä»½ å¹´ç³• ï¼Œ ç¬¬ä¸€æ¬¡ åƒç«é”… ä¼š åœ¨ æ¡Œä¸Š æœ‰ è¿™ä¹ˆ å¤š çš„ ä¸»é£Ÿ äº†\n",
      "æ•´ä½“ æ¥è¯´ è¿™å®¶ ç«é”…åº— æ²¡æœ‰ ä»€ä¹ˆ ç‰¹åˆ« æœ‰ ç‰¹è‰² çš„ ï¼Œ ä¸è¿‡ æ¯ä»½ èœå“ åˆ†é‡ è¿˜æ˜¯ æ¯”è¾ƒ è¶³ çš„ ï¼Œ è¿™ç‚¹ è¦ è‚¯å®š ï¼ è‡³äº ä»·æ ¼ ï¼Œ å› ä¸º æ²¡æœ‰ çœ‹ èœå• ä¸ äº†è§£ ï¼Œ ä¸è¿‡ æˆ‘ çœ‹ å¤§ä¼— æœ‰ è¿™å®¶ åº— çš„ å›¢è´­ ä»£é‡‘åˆ¸ ï¼Œ ç›¸å½“äº 7 æŠ˜ ï¼Œ åº”è¯¥ ä»·ä½ ä¸ä¼š å¾ˆ é«˜ çš„ ï¼ æœ€å è¿˜æ˜¯ è¦ æ„Ÿè°¢ å•†å®¶ æä¾› éœ¸ç‹é¤ ï¼Œ ç¥ ç”Ÿæ„å…´éš† ï¼Œ è´¢æº å¹¿è¿›\n"
     ]
    }
   ],
   "source": [
    "content_to_corpus(df_train, 'data/text_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_to_corpus(df_train.sample(1000, random_state=1),\n",
    "                  'data/text_train_10k.txt', print_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å“ ï¼Œ æƒ³å½“å¹´ æ¥ ä½˜å±± çš„ æ—¶å€™ ï¼Œ å•¥ éƒ½ æ²¡æœ‰ ï¼Œ ä¸‰å“ é¦™ç®— é•‡ä¸Š æœ€å¤§ çœ‹èµ·æ¥ æœ€ åƒæ · çš„ é¥­åº— äº†\n",
      "èœå“ å¤š ï¼Œ æœ‰ç‚¹ å¤ª å¤š ï¼Œ æ„Ÿè§‰ å•¥ éƒ½ æœ‰ ï¼Œ æ‚éƒ½ ä¸è¶³ä»¥ å½¢å®¹\n",
      "éšä¾¿ ç‚¹äº› ï¼Œ å±…ç„¶ å£å‘³ ä»€ä¹ˆ çš„ éƒ½ å¥½ è¿˜ å¯ä»¥ ï¼Œ ä»·é’± è‡ªç„¶ æ˜¯ ä¾¿å®œ å½“ éœ‡æƒŠ\n",
      "å…ƒå® è™¾ å’Œ æ¤’ç› ä¹è‚šé±¼ éƒ½ ä¸é”™ åƒ\n",
      "ä¸è¿‡ è¿‘æ¥ å‡ æ¬¡ ä¹ˆ ï¼Œ å‘³é“ æ˜æ˜¾ æ²¡ ä»¥å‰ å¥½ äº†\n",
      "å†·é¤ é‡Œé¢ ä¸€ä¸ª å‡‰æ‹Œ æµ·å¸¦ä¸ è¿˜ å¯ä»¥ ï¼Œ é…¸é…¸ç”œç”œ çš„\n",
      "é•‡ä¸Š ä¹Ÿ æœ‰ äº† äº› åˆ«çš„ å¤§ç‚¹ çš„ é¥­åº— ï¼Œ æ‰€ä»¥ ä¸æ˜¯ æ¯æ¬¡ å¿…æ¥ äº†\n",
      "å¯¹ äº† ï¼Œ è¿™å®¶ çš„ ç”Ÿæ„ ä¸€å¦‚æ—¢å¾€ çš„ è¶…çº§ å¥½ ï¼Œ ä¸ å®šä½ åŸºæœ¬ åƒ ä¸åˆ°\n",
      "ä¸è¿‡ ä½˜å±± è¿™è¾¹ çš„ äºº åƒæ™šé¥­ å¾ˆæ—© çš„ ï¼Œ æ‰€ä»¥ ç¨å¾® æ™šç‚¹ å» å°± å¾ˆ ç©º äº†\n",
      "è¶ç€ å›½åº†èŠ‚ ï¼Œ ä¸€å®¶äºº åœ¨ ç™½å¤© åœ¨ å±±é‡Œ ç©è€ ä¹‹å ï¼Œ æ™šä¸Š å†³å®š åƒ æè®° æ…å›¢\n"
     ]
    }
   ],
   "source": [
    "content_to_corpus(df_valid, 'data/text_valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘ æƒ³ è¯´ ä»–ä»¬ å®¶ çš„ ä¼˜æƒ æ´»åŠ¨ å¥½ æŒä¹… å•Š ï¼Œ æˆ‘ é¢„å”® çš„ æ—¶å€™ ä¹° çš„ åˆ¸ ï¼Œ å‰ä¸¤å¤© å¿ƒè¡€æ¥æ½® å» åƒ çš„ æ´»åŠ¨ è¿˜ åœ¨ ç»§ç»­\n",
      "é¦–å…ˆ è¯´ ä¸‹ æœåŠ¡ ï¼Œ å› ä¸º å’Œ ç”·ç¥¨ å¼€è½¦ å» çš„ ï¼Œ æœ‰ç‚¹ ä¸ è®¤è·¯ ï¼Œ è€æ¿ å¾ˆ è€å¿ƒ çš„ åœ¨ ç”µè¯ é‡Œ å¸® æˆ‘ä»¬ æŒ‡è·¯ ï¼Œ åˆ° äº† é—¨åº— ä¹‹å ä¹Ÿ å¸® æˆ‘ä»¬ æ¨è äº† ä»–ä»¬ å®¶ åš çš„ æ¯”è¾ƒ åœ°é“ çš„ ä¼¤å¿ƒ å‡‰ç²‰ ï¼Œ è¯´ æ˜¯ å¨å¸ˆ æ˜¯ å››å· é‚£è¾¹ æ¥ çš„\n",
      "ç¯å¢ƒ å‘¢ æ¯”è¾ƒç®€å• å¹²å‡€ ï¼Œ å» çš„ æ—¶å€™ ä¸‹åˆ ä¸€ç‚¹å¤š äº† ï¼Œ è¿˜æœ‰ å››äº”æ¡Œ äºº åœ¨ ç”¨é¤\n",
      "å£å‘³ å¯¹äº æˆ‘ è€Œè¨€ ç‚¹ äº† éº»è¾£ çš„ å£æ„Ÿ æ­£ æ­£å¥½ ï¼Œ ç”·ç¥¨ æ¯”è¾ƒ èƒ½ åƒ è¾£ ï¼Œ ç›¸å¯¹è€Œè¨€ è§‰å¾— ä»–ä»¬ å®¶ çš„ éº»è¾£ å£æ„Ÿ éº»æœ‰ äº† ï¼Œ è¾£ è¿˜ æ¬ ç¼º ä¸€ç‚¹ ï¼Œ è€æ¿å¨˜ è¯´ è€ƒè™‘ åˆ° å®¢äºº å£å‘³ ä¸åŒ æ‰€ä»¥ æ²¡æ•¢ æ”¾å¤ªå¤š è¾£æ¤’ ï¼Œ èƒ½ åƒ è¾£ çš„ æœ‹å‹ å¯ä»¥ è€ƒè™‘ ä¸‹å• ä¹‹å‰ å’Œ è€æ¿ å…ˆ è¯´å¥½\n",
      "é±¼ å‘¢ æˆ‘ä»¬ é€‰ çš„ æ˜¯ é»‘é±¼ ï¼Œ 2.9 æ–¤ çš„ é±¼ åŠ ä¸Š ä¸€ç›† æˆ‘ ä»¥ä¸º æ²¡æœ‰ ä»€ä¹ˆ ä¸œè¥¿ å®é™…ä¸Š ä¸œè¥¿ å¾ˆå¤š çš„ é”…åº• ï¼Œ æˆ‘ä»¬ åƒ çš„ é¥±é¥± çš„ ï¼Œ æœ€å ä»¥ä¸º åƒ çš„ å·®ä¸å¤š äº† ï¼Œ æ‰“åŒ… ä¸€çœ‹ ç®€ç›´ åƒ æ²¡åŠ¨ è¿‡ ä¸€æ · ï¼Œ åˆ†é‡ è¿˜æ˜¯ æ»¡è¶³ çš„ ï¼Œ é±¼ æ¯”è¾ƒ æ–°é²œ\n",
      "ä¼¤å¿ƒ å‡‰ç²‰ å¾ˆè¾£ ï¼Œ ä¸è¿‡ å£å‘³ ä¹Ÿ è›® å¥½åƒ çš„\n",
      "æ€»çš„æ¥è¯´ ï¼Œ æ€§ä»·æ¯” è¿˜æ˜¯ å¯ä»¥ çš„ ï¼Œ ä¸¤ä¸ª äºº åƒ äº† å¤§æ¦‚ 160 å·¦å³ ï¼Œ ç”¨ äº† å›¢è´­ åˆ¸ çš„è¯ ä¸€ç™¾å— ä¸åˆ° ï¼Œ ä¼š è€ƒè™‘ ä¸‹æ¬¡ å† æ¥\n",
      "ç»ˆäº å¼€ åˆ° å¿ƒå¿ƒå¿µå¿µ çš„ LAB BBLANKK loft\n",
      "ç¬¬ä¸€æ¬¡ æ¥ å°± éšä¾¿ ç‚¹ ä¹Ÿ ä¸€äº› ï½ ã€ é¦™è¾£è™¾ æ„ é¢ ã€‘ è›®è¾£ çš„ ï¼Œ ä½† å…¶å® ä¸€èˆ¬èˆ¬\n",
      "ã€ ç›æ ¼ä¸½ç‰¹ ã€‘ è¿›å£ çš„ æ„Ÿè§‰ è›® å¥½ çš„ å°±æ˜¯ å–å®Œ å å°± ç‚¹ å‘› ï½ ä½†æ˜¯ æœ‹å‹ ä¸æ˜¯ å¾ˆ å–œæ¬¢ ã€ ä¸€æŸ±æ“å¤© ã€‘ çœ‹ ç‚¹è¯„ å¾ˆå¤š äºº è¯´ å–œæ¬¢ å°± ç‚¹ äº† ï¼Œ æ°´èœœæ¡ƒ å‘³ ï¼Œ è¿˜ ä¸é”™ æŒº å¥½å– çš„ ï½ èµ ã€ æµ·é²œ é¥­ ã€‘ æƒ³ åƒé¥­ ä½† è¿™åº— çš„ é¥­ç±» åªæœ‰ ä¸¤ç§ ï¼Œ å°± ç‚¹ äº† è¿™ä¸ª\n"
     ]
    }
   ],
   "source": [
    "content_to_corpus(df_testa, 'data/text_testa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯ä»¥ è¯´ å·¥ä½œæ—¥ ä¸­åˆ çš„ è¿™ä¸ª å¥—é¤ ç€å® æ˜¯ ç‰¹åˆ« çš„ å®æƒ  å•Š ï¼Œ åº—å®¶ ç»™ çš„ é‡ ä¹Ÿ æ˜¯ å¤§å¤§çš„ è¶³ ï¼ é¦–å…ˆ å¦‚å›¾ ï¼Œ æ¯ä¸ª èœå“ æ‘†ç›˜ éƒ½ å¾ˆ ç²¾ç¾ ï¼Œ è®© äºº ä¸€ çœ‹ å°± å……æ»¡ é£Ÿæ¬²\n",
      "åœŸè±†æ³¥ å£å‘³ å¾ˆæ£’ ï¼Œ ç”œ é…±æ²¹ ã€ é±¼ç±½ å’Œ æ²™æ‹‰é…± çš„ æ­é… éå¸¸ å®Œç¾\n",
      "ä¸‰æ–‡é±¼ å¾ˆ æ–°é²œ ï¼Œ åˆ‡å¾— å¾— å¥½ åš å•Š ï¼Œ åƒ èµ·æ¥ éå¸¸ æ»¡è¶³ ï¼ çŒªæ’ é¥­ä¸Š çš„ å®åœ¨ æœ‰ç‚¹ æ…¢ äº† ï¼Œ è‡³å°‘ ç­‰ äº† 20 åˆ†é’Ÿ ï¼Œ ä½†é‡ è¶…çº§ å¤§ ï¼Œ ä¸Šé¢ æ˜¯ åˆ å¤§ åˆ åš çš„ ç‚¸ çŒªæ’ ï¼Œ ä¸ çŸ¥é“ æ˜¯ä¸æ˜¯ åšå¥½ äº† æ²¡æœ‰ åŠæ—¶ ç»™ ä¸Š ï¼Œ çŒªæ’ å·²ç» è¢« é…±æ± æ³¡å¾—å˜ æ¹¿è½¯ äº† ï¼Œ æ²¡æœ‰ äº† é…¥è„† æ„Ÿ\n",
      "è¿™ä¸ª å¥—é¤ åº—å®¶ çœŸçš„ æ˜¯ æ»¡æ»¡çš„ è¯šæ„ å•Š ï¼Œ ä¼°è®¡ ä¸€èˆ¬ äºº çœŸçš„ åƒä¸å®Œ\n",
      "ä½ç½® å¾ˆ ä¸é”™ ï¼Œ è€æ¿ çœ¼å…‰ å¾ˆ å¥½ ğŸ‘\n",
      "ä½äº åœ°é“ç«™ é™„è¿‘ ï¼Œ å‘¨å›´ æœ‰ åŠå…¬æ¥¼ ã€ å•†åœˆ ã€ ä½å®… ã€ åˆ«å¢… è™½ç„¶ ç°åœ¨ è¿˜ æœ‰äº› è’å‡‰ ï¼Œ ä½† æˆ‘ æƒ³ å¾ˆå¿« ä¼š å‘å±• èµ·æ¥ çš„ ğŸ’ª\n",
      "é¤å… å†…éƒ¨ çš„ ç¯å¢ƒ è¿˜ ç®— å¹²å‡€ æ¸©é¦¨\n",
      "æœåŠ¡å‘˜ ä¹Ÿ å¾ˆ çƒ­æƒ… äº²åˆ‡\n",
      "å§œæ²¹å¤§ èŠ¥èœ ï¼š é¦–å…ˆ çœ‹åˆ° è§‰å¾— ç•¥å¾® ç²—æ—· äº† äº› ï¼Œ ä½† åƒ èµ·æ¥ å‘³é“ ä¸é”™ ï¼Œ å§œä¸ æ²¡æœ‰ å¾ˆ æµ“é‡ çš„ å‘³é“\n",
      "é¦™ç…æµ· é²ˆé±¼ ï¼š ç‰¹åˆ« é…¥è„† é²œé¦™ å¥½åƒ ğŸ˜‹\n"
     ]
    }
   ],
   "source": [
    "content_to_corpus(df_testb, 'data/text_testb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2896571 76231079 412681013 data/text_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/text_train.txt data/text_valid.txt data/text_testa.txt data/text_testb.txt > data/text_all.txt\n",
    "!wc data/text_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dataset combined has 76 million words, which is too many. According to [ELMoForManyLangs](https://github.com/HIT-SCIR/ELMoForManyLangs), it would take 3 days to train 20m words on an NVIDIA P100 GPU.\n",
    "\n",
    "We must sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   82727 2255688 12200210 data/text_train_10k.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc data/text_train_10k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random sample of 10,000 reviews gives about 83k sentences, and 2.2m words.\n",
    "It should take probably 8 hours to train the embedings from the 10,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åº—é‡Œ å†°æ·‡æ·‹ æœ‰ 12 ä¸ª å£å‘³ çš„ ~\r\n",
      "é™¤äº† å†°æ·‡æ·‹ ~ ç”œå“ é¥®æ–™ ä¹Ÿ æœ‰ ~ çœ‹ ç”µå½± å‰ å¯ä»¥ æ¥ è¿™é‡Œ ç­‰ ~ ä¹Ÿ å¯ä»¥ ä¹° è¿›å» åƒ å“ˆ ~\r\n",
      "# æ«æ ‘ èƒ¡æ¡ƒ # BBLANKK\r\n",
      "åº—å‘˜ å¦¹å­ æŒ– å®Œ å†°æ·‡æ·‹ è¿˜ å¸® æˆ‘ ç§° äº† ä¸€ä¸‹ ~ è¯´ ä»–ä»¬ å®¶ çš„ å•çƒ æ˜¯ 60g ~\r\n",
      "å‘³é“ å¾ˆ å¥½ å•Š ~ å¥¶å‘³ç®—é‡ çš„ äº† ~ ä¹Ÿ ä¸ä¼š å¾ˆç”œ ~ é‡Œé¢ æœ‰ æ ¸æ¡ƒä» ~ æ ¸æ¡ƒä» ä¹Ÿ å¥½åƒ ~\r\n",
      "ä¸€ä¸ª çƒæŒº å¤š çš„ ~ åƒ ç€ ç»å¯¹ å¤Ÿ äº† ~\r\n",
      "ä¸è¿‡ åŸä»· æœ‰ç‚¹ å°è´µ ~ M å›¢æœ‰ å›¢è´­ èƒ½ ä¾¿å®œ ä¸€ç‚¹ ~\r\n",
      "æ€»ä½“ å¾ˆ æ»¡æ„ çš„ ~ ä»¥å æƒ³ åƒ å†°æ·‡æ·‹ å¯ä»¥ æ¥ è¿™å®¶ ~ å† åƒ åƒ åˆ«çš„ å‘³ ~\r\n",
      "æé£Ÿ urban BBLANKK harvest æ˜¯ ä¸€ä¸ª èµ° è‡ªç„¶ ç¯ä¿ é«˜ç«¯ çº¿è·¯ çš„ é¤å… ï¼Œ ä½äº æ­å· å¤§å¦ Dåº§ 5 æ¥¼ ï½ å…‰çœ‹ åœ°æ ‡ ä¹Ÿ æ˜¯ å¤Ÿ é€¼æ ¼ äº† ä¸€è¿› é—¨å£ è£…é¥° æœ‰ å¾®å‹ ç”°å›­ BBLANKK çœ‹ä¸Šå» é£Ÿæ éƒ½ æ–°é²œ çš„ ä¸è¦ ä¸è¦ çš„ ï½ ç»™ åˆ›æ„ ä¸€ä¸ª èµ BBLANKK åªè¦ èœå• ä¸Š æœ‰ â€œ å³ æ‘˜ â€ å­—çœ¼ çš„ ï¼Œ éƒ½ ç”± æœåŠ¡å‘˜ ç°åœº é‡‡æ‘˜ å–ç”¨ ï¼Œ èœè‰² æ‘†ç›˜ ç²¾è‡´ è€Œä¸” è‰²å½© ä¸°å¯Œ ï¼Œ è®© äºº é£Ÿæ¬² å¤§å¼€ ï¼Œ å£å‘³ éƒ½ è¿˜ ä¸é”™ ï½ ä¸è¿‡ æœ¬äºº æœ¬ å°± ä¸æ˜¯ ä¸€ä¸ª å¤ª æŒ‘é£Ÿ çš„ äºº å•¦ é¥­å ä¸Š äº† ç”œå“ ï¼Œ å¥½åƒ åˆ° é£ èµ·æ¥ ï¼Œ åƒ å¾— æ„‰å¿« å¿ƒæƒ… å°± å¥½å¥½ ï¼Œ å°¤å…¶ æ˜¯ è¿™æ · ä¸€ä¸ª å‘¨æœ«\r\n",
      "å› ä¸º ä»·æ ¼ åè´µ çš„ ç¼˜æ•… ï¼Œ å¦‚æ­¤ é»„é‡‘åœ°æ®µ ã€ é«˜å³° æ—¶é—´ ç”¨é¤ äºº å€’ ä¸ å¤š ï¼Œ è¿™ ä¹Ÿ ä¿è¯ äº† ç”¨é¤ ç¯å¢ƒ çš„ èˆ’é€‚åº¦ ï¼Œ è¿˜æœ‰ ä¸€ä¸ª å¥½æ¶ˆæ¯ å°±æ˜¯ ï¼š å¼€ä¸š åˆæœŸ æœ‰ ä¸ƒæŠ˜ ä¼˜æƒ  å“¦ ï¼ å¤§å®¶ èµ¶ç´§ å» æ‹”è‰ å§\r\n"
     ]
    }
   ],
   "source": [
    "!tail data/text_train_10k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's try a very small subset just to make sure the software works.\n",
    "\n",
    "Use 1000 sentences as training and 100 sentences as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1000   27705  150876 data/text_train_1ks.txt\n",
      "     100    2729   14924 data/text_train_100s.txt\n"
     ]
    }
   ],
   "source": [
    "# !shuf -n 10000 data/all_text.txt > data/little_text.txt\n",
    "# Use `gshuf` on Mac\n",
    "!gshuf -n 1000 data/text_train.txt > data/text_train_1ks.txt\n",
    "!gshuf -n 100 data/text_train.txt > data/text_train_100s.txt\n",
    "!wc data/text_train_1ks.txt\n",
    "!wc data/text_train_100s.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, clip_grad=5.0, config_path='/Users/jesse/workspace/ML/fine-grain-sentiment-analysis/misc/elmo/cnn_50_100_512_4096_sample.json', eval_steps=10000, gpu=0, lr=0.001, lr_decay=0.8, max_epoch=10, max_sent_len=20, max_vocab_size=150000, min_count=5, model='data/elmo-zhs-1k', optimizer='adam', save_classify_layer=False, seed=1, test_path=None, train_path='data/text_train_100s.txt', valid_path=None, valid_size=10, word_embedding=None)\n",
      "{'encoder': {'name': 'elmo', 'projection_dim': 512, 'cell_clip': 3, 'proj_clip': 3, 'dim': 4096, 'n_layers': 2}, 'token_embedder': {'name': 'cnn', 'activation': 'relu', 'filters': [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], 'n_highway': 2, 'word_dim': 100, 'char_dim': 50, 'max_characters_per_token': 50}, 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192}, 'dropout': 0.1}\n",
      "2018-11-15 03:06:43,847 INFO: training instance: 149, training tokens: 2836.\n",
      "2018-11-15 03:06:43,847 INFO: training instance: 139, training tokens after division: 2646.\n",
      "2018-11-15 03:06:43,847 INFO: valid instance: 10, valid tokens: 190.\n",
      "2018-11-15 03:06:43,850 INFO: Truncated word count: 1359.\n",
      "2018-11-15 03:06:43,850 INFO: Original vocabulary size: 1090.\n",
      "2018-11-15 03:06:43,851 INFO: Word embedding size: 78\n",
      "2018-11-15 03:06:43,853 INFO: Char embedding size: 902\n",
      "2018-11-15 03:06:44,055 INFO: 5 batches, avg len: 20.0\n",
      "2018-11-15 03:06:44,056 INFO: Evaluate every 10000 batches.\n",
      "2018-11-15 03:06:44,067 INFO: 1 batches, avg len: 20.0\n",
      "2018-11-15 03:06:44,067 INFO: vocab size: 78\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2018-11-15 03:06:49,848 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(78, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(902, 50, padding_idx=899)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (classify_layer): SampledSoftmaxLayer(\n",
      "    (criterion): CrossEntropyLoss()\n",
      "    (column_emb): Embedding(78, 512)\n",
      "    (column_bias): Embedding(78, 1)\n",
      "  )\n",
      ")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:343: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:348: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:297: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "2018-11-15 03:06:55,393 INFO: Epoch=0 iter=5 lr=0.001000 valid_ppl=72.815895\n",
      "2018-11-15 03:06:55,994 INFO: New record achieved!\n",
      "2018-11-15 03:06:58,477 INFO: Epoch=1 iter=5 lr=0.000800 valid_ppl=38.599525\n",
      "2018-11-15 03:06:59,102 INFO: New record achieved!\n",
      "2018-11-15 03:07:01,467 INFO: Epoch=2 iter=5 lr=0.000640 valid_ppl=14.500859\n",
      "2018-11-15 03:07:02,077 INFO: New record achieved!\n",
      "2018-11-15 03:07:05,598 INFO: Epoch=3 iter=5 lr=0.000512 valid_ppl=11.034621\n",
      "2018-11-15 03:07:06,522 INFO: New record achieved!\n",
      "2018-11-15 03:07:08,950 INFO: Epoch=4 iter=5 lr=0.000410 valid_ppl=11.360459\n",
      "2018-11-15 03:07:11,265 INFO: Epoch=5 iter=5 lr=0.000328 valid_ppl=8.874183\n",
      "2018-11-15 03:07:11,868 INFO: New record achieved!\n",
      "2018-11-15 03:07:14,915 INFO: Epoch=6 iter=5 lr=0.000262 valid_ppl=8.629713\n",
      "2018-11-15 03:07:15,532 INFO: New record achieved!\n",
      "2018-11-15 03:07:17,992 INFO: Epoch=7 iter=5 lr=0.000210 valid_ppl=8.306626\n",
      "2018-11-15 03:07:18,607 INFO: New record achieved!\n",
      "2018-11-15 03:07:20,959 INFO: Epoch=8 iter=5 lr=0.000168 valid_ppl=8.456163\n",
      "2018-11-15 03:07:23,581 INFO: Epoch=9 iter=5 lr=0.000134 valid_ppl=8.405726\n",
      "2018-11-15 03:07:23,584 INFO: best train ppl: 100000000.000000, best valid ppl: 8.306626.\n"
     ]
    }
   ],
   "source": [
    "!python -m elmoformanylangs.biLM train \\\n",
    "    --train_path data/text_train_100s.txt \\\n",
    "    --model data/elmo-zhs-1k \\\n",
    "    --valid_size 10 \\\n",
    "    --config_path `pwd`/misc/elmo/cnn_50_100_512_4096_sample.json \\\n",
    "    --seed 1 \\\n",
    "    --gpu 0 \\\n",
    "    --optimizer adam \\\n",
    "    --lr 0.001 \\\n",
    "    --lr_decay 0.8 \\\n",
    "    --batch_size 32 \\\n",
    "    --clip_grad 5 \\\n",
    "    --max_epoch 10 \\\n",
    "    --max_sent_len 20 \\\n",
    "    --max_vocab_size 150000 \\\n",
    "    --eval_steps 10000 \\\n",
    "    --min_count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, clip_grad=5.0, config_path='/Users/jesse/workspace/ML/fine-grain-sentiment-analysis/misc/elmo/cnn_50_100_512_4096_sample.json', eval_steps=10000, gpu=0, lr=0.001, lr_decay=0.8, max_epoch=10, max_sent_len=20, max_vocab_size=150000, min_count=5, model='data/elmo-zhs-1k', optimizer='adam', save_classify_layer=False, seed=1, test_path=None, train_path='data/text_train_1k.txt', valid_path='data/text_train_100.txt', valid_size=0, word_embedding=None)\n",
      "{'encoder': {'name': 'elmo', 'projection_dim': 512, 'cell_clip': 3, 'proj_clip': 3, 'dim': 4096, 'n_layers': 2}, 'token_embedder': {'name': 'cnn', 'activation': 'relu', 'filters': [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]], 'n_highway': 2, 'word_dim': 100, 'char_dim': 50, 'max_characters_per_token': 50}, 'classifier': {'name': 'sampled_softmax', 'n_samples': 8192}, 'dropout': 0.1}\n",
      "2018-11-14 22:36:57,640 INFO: training instance: 1505, training tokens: 28596.\n",
      "2018-11-14 22:36:57,645 INFO: valid instance: 143, valid tokens: 2705.\n",
      "2018-11-14 22:36:57,675 INFO: Truncated word count: 7085.\n",
      "2018-11-14 22:36:57,676 INFO: Original vocabulary size: 5629.\n",
      "2018-11-14 22:36:57,683 INFO: Word embedding size: 786\n",
      "2018-11-14 22:36:57,704 INFO: Char embedding size: 2140\n",
      "2018-11-14 22:36:59,727 INFO: 48 batches, avg len: 20.0\n",
      "2018-11-14 22:36:59,728 INFO: Evaluate every 10000 batches.\n",
      "2018-11-14 22:37:00,040 INFO: 5 batches, avg len: 19.9\n",
      "2018-11-14 22:37:00,040 INFO: vocab size: 786\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2018-11-14 22:37:08,984 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(786, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(2140, 50, padding_idx=2137)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (classify_layer): SampledSoftmaxLayer(\n",
      "    (criterion): CrossEntropyLoss()\n",
      "    (column_emb): Embedding(786, 512)\n",
      "    (column_bias): Embedding(786, 1)\n",
      "  )\n",
      ")\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:343: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:348: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "2018-11-14 22:37:40,645 INFO: Epoch=0 iter=32 lr=0.001000 train_ppl=72388.250000 time=28.90s\n",
      "/Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py:297: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "2018-11-14 22:37:56,929 INFO: Epoch=0 iter=48 lr=0.001000 valid_ppl=58.615864\n",
      "2018-11-14 22:37:57,981 INFO: New record achieved!\n",
      "2018-11-14 22:38:24,576 INFO: Epoch=1 iter=32 lr=0.000800 train_ppl=63.343208 time=26.59s\n",
      "2018-11-14 22:38:42,185 INFO: Epoch=1 iter=48 lr=0.000800 valid_ppl=44.460205\n",
      "2018-11-14 22:38:42,798 INFO: New record achieved!\n",
      "2018-11-14 22:39:09,410 INFO: Epoch=2 iter=32 lr=0.000640 train_ppl=52.716682 time=26.61s\n",
      "2018-11-14 22:39:24,734 INFO: Epoch=2 iter=48 lr=0.000640 valid_ppl=41.916843\n",
      "2018-11-14 22:39:25,305 INFO: New record achieved!\n",
      "2018-11-14 22:39:55,008 INFO: Epoch=3 iter=32 lr=0.000512 train_ppl=47.527332 time=29.70s\n",
      "2018-11-14 22:40:11,341 INFO: Epoch=3 iter=48 lr=0.000512 valid_ppl=39.813450\n",
      "2018-11-14 22:40:12,012 INFO: New record achieved!\n",
      "2018-11-14 22:40:38,603 INFO: Epoch=4 iter=32 lr=0.000410 train_ppl=43.219276 time=26.58s\n",
      "2018-11-14 22:40:54,555 INFO: Epoch=4 iter=48 lr=0.000410 valid_ppl=38.112942\n",
      "2018-11-14 22:40:55,184 INFO: New record achieved!\n",
      "2018-11-14 22:41:27,812 INFO: Epoch=5 iter=32 lr=0.000328 train_ppl=40.482231 time=32.62s\n",
      "2018-11-14 22:41:44,222 INFO: Epoch=5 iter=48 lr=0.000328 valid_ppl=37.239307\n",
      "2018-11-14 22:41:44,942 INFO: New record achieved!\n",
      "2018-11-14 22:42:12,696 INFO: Epoch=6 iter=32 lr=0.000262 train_ppl=36.799458 time=27.75s\n",
      "2018-11-14 22:42:29,051 INFO: Epoch=6 iter=48 lr=0.000262 valid_ppl=36.741238\n",
      "2018-11-14 22:42:29,662 INFO: New record achieved!\n",
      "2018-11-14 22:42:58,419 INFO: Epoch=7 iter=32 lr=0.000210 train_ppl=35.172501 time=28.75s\n",
      "2018-11-14 22:43:17,635 INFO: Epoch=7 iter=48 lr=0.000210 valid_ppl=36.912933\n",
      "2018-11-14 22:43:48,403 INFO: Epoch=8 iter=32 lr=0.000168 train_ppl=32.478752 time=30.75s\n",
      "2018-11-14 22:44:03,723 INFO: Epoch=8 iter=48 lr=0.000168 valid_ppl=36.847874\n",
      "2018-11-14 22:44:29,567 INFO: Epoch=9 iter=32 lr=0.000134 train_ppl=31.984341 time=25.84s\n",
      "2018-11-14 22:44:51,497 INFO: Epoch=9 iter=48 lr=0.000134 valid_ppl=37.348961\n",
      "2018-11-14 22:44:51,503 INFO: best train ppl: 100000000.000000, best valid ppl: 36.741238.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# add `pwd` to `config_path` because trained model depends on that file\n",
    "# absolute path makes it easier to find\n",
    "!python -m elmoformanylangs.biLM train \\\n",
    "    --train_path data/text_train_1ks.txt \\\n",
    "    --valid_path data/text_train_100s.txt \\\n",
    "    --model data/elmo-zhs-1k \\\n",
    "    --config_path `pwd`/misc/elmo/cnn_50_100_512_4096_sample.json \\\n",
    "    --seed 1 \\\n",
    "    --gpu 0 \\\n",
    "    --optimizer adam \\\n",
    "    --lr 0.001 \\\n",
    "    --lr_decay 0.8 \\\n",
    "    --batch_size 32 \\\n",
    "    --clip_grad 5 \\\n",
    "    --max_epoch 10 \\\n",
    "    --max_sent_len 20 \\\n",
    "    --max_vocab_size 150000 \\\n",
    "    --eval_steps 10000 \\\n",
    "    --min_count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-14 23:19:31,908 [INFO] Read cache data/train/sentiment_analysis_trainingset.csv.segged_sample_None.tsv..\n"
     ]
    }
   ],
   "source": [
    "df_train = read_csv(train_data_path, seg_words=True, sample_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-14 23:21:26,090 [INFO] char embedding size: 8582\n",
      "2018-11-14 23:21:26,861 [INFO] word embedding size: 69827\n",
      "2018-11-14 23:21:34,862 [INFO] Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(69827, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(8582, 50, padding_idx=8579)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2018-11-14 23:21:35,856 [INFO] 1 batches, avg len: 371.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.16197295,  0.05117923, -0.13360588, ...,  0.24924016,\n",
       "         -0.27383432, -0.05000202],\n",
       "        [ 0.06199374,  0.07696173, -0.23577517, ...,  0.26290917,\n",
       "         -0.27809614, -0.06464732],\n",
       "        [ 0.0008731 , -0.04712557, -0.08987609, ..., -0.00968068,\n",
       "         -0.13784361, -0.03070146],\n",
       "        ...,\n",
       "        [-0.27461815,  0.03719755, -0.13428287, ..., -0.06027537,\n",
       "         -0.07961495, -0.14139868],\n",
       "        [-0.06108154,  0.13908525, -0.2571093 , ...,  0.01645198,\n",
       "         -0.02265188, -0.02150639],\n",
       "        [-0.19907278,  0.02419554, -0.19596738, ...,  0.07171986,\n",
       "         -0.01507132,  0.02613006]], dtype=float32),\n",
       " array([[-0.17354196, -0.36698878, -0.04885902, ..., -0.02680009,\n",
       "          0.13608061,  0.12091058],\n",
       "        [-0.29550084, -0.12204532, -0.28084293, ..., -0.29321548,\n",
       "          0.13516046, -0.11846065],\n",
       "        [-0.05146622, -0.19961458,  0.01509347, ...,  0.05483557,\n",
       "         -0.06374971, -0.0203303 ],\n",
       "        ...,\n",
       "        [-0.1181826 ,  0.02767085, -0.35975978, ..., -0.01324531,\n",
       "          0.00909343, -0.03078758],\n",
       "        [-0.0251124 ,  0.20625134, -0.2706751 , ...,  0.09297368,\n",
       "          0.14488776, -0.00088627],\n",
       "        [-0.04889162, -0.19273376, -0.3477179 , ...,  0.0895622 ,\n",
       "          0.04090813,  0.11943939]], dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elmoformanylangs import Embedder\n",
    "\n",
    "elmoemb = Embedder('data/elmo-zhs-fsauor')\n",
    "# e = Embedder('data/elmo-zhs-1k')\n",
    "\n",
    "sents = df_train['content'][0:2]\n",
    "elmoemb.sents2elmo(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's train a bigger model.\n",
    "\n",
    "I'm running this on my 16G Macbook.\n",
    "10k reviews (82727 lines) takes about 3G memory.\n",
    "40k reviews (344k lines) take about 6.4G memory.\n",
    "\n",
    "My usable memory is about 10G, which should be able to\n",
    "handle 100K reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿™å®¶ åº— è¿˜ æ²¡æœ‰ å¼€ä¸š çš„ æ—¶å€™ å°± æå‰ åœ¨ ç½‘ä¸Š æœ‰ å”®å– ï¼Œ å½“æ—¶ æŠ± ç€ ä¸ç®¡ åƒ ä¸åƒ ï¼Œ å…ˆ æŠ¢ ä¸¤ä»½ çš„ å¿ƒæ€ å°± ä¸‹å• ä¹° äº† ä¸¤ä»½\n",
      "# å¼€åº— åˆ° ç°åœ¨ éƒ½ å¿« å¤§åŠå¹´ äº† ï¼Œ ä¸€ç›´ éƒ½ æ²¡æœ‰ å» ä½¿ç”¨ ï¼Œ æ å¾— éƒ½ å¿« è¿‡æœŸ äº† ï¼Œ ä»Šå¤© å®¶é‡Œ åˆ åœæ°´ äº† å°± æƒ³ç€ è¿‡æ¥ æŠŠ è¿™ä¸ª ç”¨ äº† å§\n",
      "O ( âˆ© _ âˆ© ) O å“ˆå“ˆ ~\n",
      "# ä½ç½® ä¹Ÿ è¿˜æ˜¯ è›® å¥½æ‰¾ çš„ å°± åœ¨ u é¢‘é“ çš„ æ˜Ÿå…‰ å¤©åœ° ï¼Œ é‡Œé¢ ä¹Ÿ æ¯”è¾ƒ å¤§ æ¯”è¾ƒ å®‰é™ ï¼Œ å“ˆå“ˆ ï¼Œ æˆ‘ä»¬ ä»Šå¤© æ¥ çš„ ä¼¼ä¹ æœ‰ç‚¹ æ—© ï¼Œ åº— é‡Œé¢ å°± æˆ‘ä»¬ ä¸€æ¡Œ ï¼\n",
      "# æŠŠ å›¢è´­ éªŒè¯ å ç‰›æ’ å¾ˆå¿« å°± å¿« å°± ä¸Šæ¥ äº† ï¼Œ åœ¨ è€æ¿ çš„ å»ºè®® ä¸‹è¦ äº† ä¸ƒåˆ† ç†Ÿ ï¼Œ ç‰›æ’ å¾ˆå«© ï¼Œ æ¯”è¾ƒ å¥½åˆ‡ ï¼\n",
      "# å¥³å„¿ ä¸ä¼š åˆ‡ ç‰›æ’ è€æ¿ è¿˜ äº²è‡ª ä¸Šé˜µ å¸® å¥³å„¿ åˆ‡ ç‰›æ’ ï¼Œ è¿˜ é€ äº† ä¸€ä¸ª æ°´æ¯ ç»™ å¥³å„¿ ï¼Œ çœŸå¿ƒ ä¸º è¿™ä¸ª ç»†å¿ƒ çš„ è€æ¿ ç‚¹ä¸ª èµ\n",
      "â€¦ â€¦ åƒ å®Œ ç‰›æ’ æ—è¾¹ è¿˜æœ‰ ä¸€äº› ç‚¹å¿ƒ å’Œ æ°´æœ ã€ å’Œ å°åƒ ã€ è¿˜æœ‰ ä¸€äº› æ°´æœ æ²™æ‹‰ ï¼Œ è‡ªé€‰ çš„ ç§ç±» ä¸æ˜¯ å¾ˆå¤š\n",
      "ï¼Œ ä½† å¯¹ è¿™ä¸ª ä»·ä½ æ¥è¯´ å·²ç» å¤Ÿ äº†\n",
      "# æ¨è ä¸€ä¸‹ é‚£ä¸ª â€œ ç‰ç±³æµ“æ±¤ â€ å¾ˆ å¼€èƒƒ çš„ ï¼Œ è¿˜æœ‰ æˆ‘ è¶…çº§ çˆ± çš„ â€œ çº¢çƒ§ çŒªè¹„ â€ åƒ äº† å¥½å¤š å‘€ ï¼ åˆ° æœ€å çœŸçš„ æ˜¯ æ‰¶ å¢™ è€Œ å‡º O ( âˆ© _ âˆ© ) O å“ˆå“ˆ ~\n",
      "# æœ€å æ€»çš„æ¥è¯´ æ¯”è¾ƒ ç»æµ å®æƒ  ï¼Œ ä¸é”™ ï¼ è€Œä¸” æœåŠ¡æ€åº¦ çœŸå¿ƒ ä¸é”™ ï¼ ï¼ ä¸‹æ¬¡ æœ‰ æœºä¼š è¿˜ä¼š å† å» çš„ ï¼Œ ä¹Ÿ ä¼š æ¨è ç»™ æœ‹å‹ ä»¬ â€¦ â€¦\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat([df_train, df_valid, df_testa, df_testb], axis=0)\n",
    "df_100k = df_all.sample(100000, random_state=1024)\n",
    "content_to_corpus(df_100k, 'data/text_100k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  864508 22698437 122895547 data/text_100k.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc data/text_100k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train the full model\n",
    "!time python -m elmoformanylangs.biLM train \\\n",
    "    --train_path data/text_100k.txt \\\n",
    "    --valid_size 50000 \\\n",
    "    --model data/elmo-zhs-100k \\\n",
    "    --config_path `pwd`/misc/elmo/cnn_50_100_512_4096_sample.json \\\n",
    "    --seed 1 \\\n",
    "    --gpu 0 \\\n",
    "    --optimizer adam \\\n",
    "    --lr 0.001 \\\n",
    "    --lr_decay 0.8 \\\n",
    "    --batch_size 32 \\\n",
    "    --clip_grad 5 \\\n",
    "    --max_epoch 10 \\\n",
    "    --max_sent_len 20 \\\n",
    "    --max_vocab_size 150000 \\\n",
    "    --eval_steps 5000 \\\n",
    "    --min_count 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: /Users/jesse/anaconda3/envs/idp/lib/python3.6/site-packages/elmoformanylangs-0.0.2-py3.6.egg/elmoformanylangs/biLM.py\r\n",
      "       [-h] [--seed SEED] [--gpu GPU] --train_path TRAIN_PATH\r\n",
      "       [--valid_path VALID_PATH] [--test_path TEST_PATH] --config_path\r\n",
      "       CONFIG_PATH [--word_embedding WORD_EMBEDDING]\r\n",
      "       [--optimizer {sgd,adam,adagrad}] [--lr LR] [--lr_decay LR_DECAY]\r\n",
      "       --model MODEL [--batch_size BATCH_SIZE] [--max_epoch MAX_EPOCH]\r\n",
      "       [--clip_grad CLIP_GRAD] [--max_sent_len MAX_SENT_LEN]\r\n",
      "       [--min_count MIN_COUNT] [--max_vocab_size MAX_VOCAB_SIZE]\r\n",
      "       [--save_classify_layer] [--valid_size VALID_SIZE]\r\n",
      "       [--eval_steps EVAL_STEPS]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --seed SEED           The random seed.\r\n",
      "  --gpu GPU             Use id of gpu, -1 if cpu.\r\n",
      "  --train_path TRAIN_PATH\r\n",
      "                        The path to the training file.\r\n",
      "  --valid_path VALID_PATH\r\n",
      "                        The path to the development file.\r\n",
      "  --test_path TEST_PATH\r\n",
      "                        The path to the testing file.\r\n",
      "  --config_path CONFIG_PATH\r\n",
      "                        the path to the config file.\r\n",
      "  --word_embedding WORD_EMBEDDING\r\n",
      "                        The path to word vectors.\r\n",
      "  --optimizer {sgd,adam,adagrad}\r\n",
      "                        the type of optimizer: valid options=[sgd, adam,\r\n",
      "                        adagrad]\r\n",
      "  --lr LR               the learning rate.\r\n",
      "  --lr_decay LR_DECAY   the learning rate decay.\r\n",
      "  --model MODEL         path to save model\r\n",
      "  --batch_size BATCH_SIZE, --batch BATCH_SIZE\r\n",
      "                        the batch size.\r\n",
      "  --max_epoch MAX_EPOCH\r\n",
      "                        the maximum number of iteration.\r\n",
      "  --clip_grad CLIP_GRAD\r\n",
      "                        the tense of clipped grad.\r\n",
      "  --max_sent_len MAX_SENT_LEN\r\n",
      "                        maximum sentence length.\r\n",
      "  --min_count MIN_COUNT\r\n",
      "                        minimum word count.\r\n",
      "  --max_vocab_size MAX_VOCAB_SIZE\r\n",
      "                        maximum vocabulary size.\r\n",
      "  --save_classify_layer\r\n",
      "                        whether to save the classify layer\r\n",
      "  --valid_size VALID_SIZE\r\n",
      "                        size of validation dataset when there's no valid.\r\n",
      "  --eval_steps EVAL_STEPS\r\n",
      "                        report every xx batches.\r\n"
     ]
    }
   ],
   "source": [
    "!python -m elmoformanylangs.biLM train -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elmoformanylangs import biLM\n",
    "\n",
    "corpus = biLM.read_corpus('data/text_100k.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter()\n",
    "for sentence in corpus:\n",
    "    word_count.update(sentence)\n",
    "\n",
    "word_count = word_count.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which `min_count` is a reasonable number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166345\n",
      "[('æ‚é²œ', 4), ('åˆ€åˆ€', 4), ('606', 4), ('453', 4), ('Wine', 4), ('æ°´ç”µ', 4), ('æ–¯æ˜¯', 4), ('æµ·æ´‹å…¬å›­', 4), ('åšå¤–', 4), ('è¿™é¡¿é¤', 4), ('å°å ¡', 4), ('å¶åœ¨', 4), ('è‚è…°', 4), ('å«£è‰²', 4), ('æœåŠ¡é¡¹ç›®', 4), ('æ„Ÿè¶³', 4), ('é¦–æ˜ ', 4), ('è•‰èŠ', 4), ('ç©ºåˆ°', 4), ('ç»™äº›', 4), ('å¤§çˆ±æ¤’', 4), ('å´è´¢è®°', 4), ('é™†å±…è·¯', 4), ('å—å§œ', 4), ('æœ‰é£Ÿ', 4), ('è©¦åƒ', 4), ('é´¨æŒ', 4), ('ç›¸é–“', 4), ('ä¸¤è§’', 4), ('çº¢ç­¾', 4), ('å®¢éš†', 4), ('ç æ‰', 4), ('ä¼šçƒ§', 4), ('ç”¨ä¸Š', 4), ('è£…ç¢Ÿ', 4), ('ç¬”ç­’', 4), ('å…ƒçœŸ', 4), ('é€Ÿå…«', 4), ('æ•°å', 4), ('ä¸¤å¤§ä¸ª', 4), ('ä¸ƒæ¬¡', 4), ('æ¯”ç™¾ä¸½åº—', 4), ('æ²¡è¯„', 4), ('å‡¤äºŒåº—', 4), ('æ¸¸å­', 4), ('ç°æ‰“ç°', 4), ('è¾ƒé…¸', 4), ('é…’é£Ÿ', 4), ('å®ä¸½é‡‘', 4), ('è¯ç­’', 4), ('æ¯›å®¶æ¹¾', 4), ('é»„æ¢…', 4), ('åŒæ¤’çŸ³', 4), ('æ‰¾å€‹', 4), ('ç•¶æ™š', 4), ('Christmas', 4), ('å‹¾å‡º', 4), ('è‘±æ¡', 4), ('è…°æœä»', 4), ('æˆ‘å˜', 4), ('åºŠå«', 4), ('çŒ®å‡º', 4), ('å…¨å®´', 4), ('å„ç•Œ', 4), ('é•¿ä¸å¤§', 4), ('å€©å½±', 4), ('å–œè›‹', 4), ('å¾ˆé€‚', 4), ('ç¼ºå¿ƒçœ¼', 4), ('é…ä¸Šå»', 4), ('ç™¾æ¡Œ', 4), ('ğŸ‘¶', 4), ('åº§åŒº', 4), ('ç»´å°¼ç†Š', 4), ('ä¼˜ä¹', 4), ('èª¿é…', 4), ('ç©ºå…³', 4), ('é“¶èŠ±', 4), ('ğŸ‹', 4), ('æ…¢æ€§å­', 4), ('å¡”åº•', 4), ('ä¸€ä¸ªç­', 4), ('æ™šè‡ªä¹ ', 4), ('å¼€é“º', 4), ('å¤šä¸‡', 4), ('æ´—è¡£', 4), ('1981', 4), ('åœ†æ–¹', 4), ('ä¹é¾™æ¹¾', 4), ('ç™¾ç¦', 4), ('å«ä½œ', 4), ('ç•¥æ¶©', 4), ('ä»·è´µé‡', 4), ('é‡‘å›', 4), ('å¾—è„†', 4), ('éµç«™', 4), ('æ•¸é‡', 4), ('åœ°å°‘', 4), ('å…©æ¨£', 4), ('éŒ¯é', 4), ('å¤šçº¿', 4), ('åŠé¤', 4), ('æ´—è„¸', 4), ('æˆ‘åˆ°', 4), ('å¼€æ˜¯', 4), ('åŠ èŒ', 4), ('æ–°é€ ', 4), ('åŒå­¸å€‘', 4), ('ç¬‹å¶', 4), ('é‡‘å…°', 4), ('çŒè¿›', 4), ('ç”œé†‹', 4), ('è—ä¸ä½', 4), ('è§‚æ¾œ', 4), ('å°å…°', 4), ('åç›–', 4), ('å†·é¹…è‚', 4), ('å°éº»å··', 4), ('å¯ç©', 4), ('è·Ÿè¿›å»', 4), ('ä¼šè®¢', 4), ('ä¸è®¢', 4), ('æ«é•‡', 4), ('è“¬å‹ƒ', 4), ('é¥ºæ˜¯', 4), ('æ¬ è€ƒè™‘', 4), ('æ²¡çª—', 4), ('æ¥ä½', 4), ('å›°äº†', 4), ('æ²¡åš¼å¤´', 4), ('ç²’é¥­', 4), ('å¤ªéªš', 4), ('åˆ«ç¢°', 4), ('ä¸å‘³', 4), ('å§œæ°´', 4), ('äº”é»‘', 4), ('èŠ±è›¤å€’', 4), ('æ€€ç”Ÿè®°', 4), ('20150904', 4), ('ä¸ºä½³', 4), ('èµ¶æ½®æµ', 4), ('æœ¨å…°', 4), ('å…¥å·', 4), ('èµ«èµ«', 4), ('ç¥åŠŸ', 4), ('å‘¨æ€»ç†', 4), ('åŒç…®', 4), ('åƒé¹…è‚', 4), ('ç¢§æ³¢', 4), ('é˜³å®¶', 4), ('8.6', 4), ('åŠ²å¤Ÿ', 4), ('æ³ªç‚¹', 4), ('åªå¤šä¸å°‘', 4), ('çŒ–ç‹‚', 4), ('å‚»å‘µå‘µ', 4), ('æ‰€å¥½', 4), ('åŠ å’¯', 4), ('æ–¹ç‰©', 4), ('æŒ‡åˆ°', 4), ('é£Ÿæå¤–', 4), ('è›®å¤§é¢—', 4), ('79.9', 4), ('è„‚è‚ªè‚', 4), ('ä¸å¤§ä¸Š', 4), ('å·¨æ²¹', 4), ('æœ‰ç”¨å—', 4), ('è­¦ç¤ºç‰Œ', 4), ('åŠæ»¡', 4), ('å¼ åŒ…', 4), ('ç®¡åƒ', 4), ('å†å·', 4), ('ç”šäº', 4), ('é˜´éƒ', 4), ('å…»å¾—', 4), ('å¥¶ä¼š', 4), ('é¦–è‚¯', 4), ('ç½‘çº¢é£', 4), ('æ…•æ–¯ä½“', 4), ('å°å¦®å­', 4), ('ä¸€ä¸ªäº¿', 4), ('å¤©æ°”é¢„æŠ¥', 4), ('è¦åŠç³–', 4), ('å–ä¸æ‰', 4), ('èŠå¤©è®°å½•', 4), ('æ¯«æ— é¡¾å¿Œ', 4), ('May', 4), ('éš¾è§', 4), ('ä¼šé£Ÿ', 4), ('åŠ¡å®', 4), ('950', 4), ('ç†Ÿåˆ°', 4), ('èµå‘³æœŸ', 4), ('å®Œå¸¦', 4), ('å¾€è¿‡', 4), ('æ¢¯é˜Ÿ', 4), ('å¼€è¡—', 4), ('é±¼å‡‰', 4), ('å¥³å¨ƒå„¿', 4), ('æ©¡ç­‹', 4)]\n"
     ]
    }
   ],
   "source": [
    "max_to_keep = 4\n",
    "\n",
    "for i, x in enumerate(word_count):\n",
    "    if x[1] <= max_to_keep:\n",
    "        break\n",
    "print(len(word_count[i:]))\n",
    "print(word_count[i:i+200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we emperically check whether the words make sense or relevant to sentiment analysis.\n",
    "If for a cutoff, most words are actual words, then we'll keep it.\n",
    "\n",
    "## Check Word Embedding Performance\n",
    "\n",
    "See `4_Word Embeddings.ipyn`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
